{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages einlesen\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data_utilitarian_mouse = pd.read_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/DataPreperation/Meta_Mouse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting special characters and review with less than 3 words\n",
    "\n",
    "# Define the regular expression pattern to match special characters\n",
    "special_characters_pattern = r'👍|:\\)'\n",
    "\n",
    "# Filter rows where 'text' column contains special characters and keep the negation\n",
    "data_utilitarian_mouse = data_utilitarian_mouse[~data_utilitarian_mouse['text'].str.contains(special_characters_pattern, regex=True)]\n",
    "\n",
    "# Now, data_utilitarian_mouse contains only rows where the 'text' column does not contain special characters\n",
    "\n",
    "\n",
    "# Count the number of words in each row of the 'text' column\n",
    "word_counts = data_utilitarian_mouse['text'].str.split().str.len()\n",
    "\n",
    "# Filter rows where the word count is greater than 2\n",
    "data_utilitarian_mouse = data_utilitarian_mouse[word_counts > 2]\n",
    "\n",
    "# Now, filtered_data_utilitarian_mouse contains only rows where the 'text' column contains more than two words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'text' column is exactly \"Es\"\n",
    "es_pattern = r'\\btodo \\b'\n",
    "data_utilitarian_mouse = data_utilitarian_mouse[~data_utilitarian_mouse['text'].str.contains(es_pattern)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the string \"[[Video ID:...]]\" from the review text\n",
    "\n",
    "# Initialize an empty list to store VideoID and numbers\n",
    "video_id_and_number_list = []\n",
    "\n",
    "# Define the regular expression pattern to match the [[VIDEOID:...]] text and extract the VideoID and numbers\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([^\\]]*)\\]\\]'\n",
    "\n",
    "# Extract VideoID and numbers from each row in the 'text' column and save them into the list\n",
    "for text in data_utilitarian_mouse['text']:\n",
    "    matches = re.findall(video_id_and_number_pattern, text)\n",
    "    for match in matches:\n",
    "        video_id_and_number_list.append('[[VIDEOID:' + match +\"]]\")\n",
    "\n",
    "# Use list to iterate through the 'text' column and replace the VideoID and numbers with an empty string\n",
    "for video_id_and_number in video_id_and_number_list:\n",
    "    data_utilitarian_mouse['text'] = data_utilitarian_mouse['text'].str.replace(video_id_and_number, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to preprocess text using spaCy with lemmatization and lowercasing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization and lowercasing\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc])\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding another column, that filters out stop words and punctuation/symbols\n",
    "import string\n",
    "\n",
    "# Define a set of stop words\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Function to preprocess text using spaCy\n",
    "def preprocess_text_stops(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization, lowercasing, and removal of symbols, punctuation, and stop words\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and not token.is_punct])\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the \"text\" column\n",
    "data_utilitarian_mouse['text_cleaned'] = data_utilitarian_mouse['text'].apply(preprocess_text)\n",
    "\n",
    "# Apply the preprocessing function with stopword removal to the \"text\" column\n",
    "data_utilitarian_mouse['text_cleaned1'] = data_utilitarian_mouse['text'].apply(preprocess_text_stops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sentiment Analysis for Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained BERT model and tokenizer\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Function to analyze the sentiment of a text\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Perform sentiment analysis\n",
    "    outputs = sentiment_model(**inputs)\n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits)\n",
    "    return predicted_label.item()  # Return the predicted label as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the utilitarian product\n",
    "data_utilitarian_mouse['sentiment'] = data_utilitarian_mouse['text'].apply(analyze_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utilitarian_mouse.to_csv('data_utilitarian_mouse_Senti.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
