{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages einlesen\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data = pd.read_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/Final Data/Utilitarian_Final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/Final Data/Utilitarian_Final.csv')\n",
    "#data_hedonic_raw = pd.read_csv('/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/Final Data/Hedonic_Final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting special characters and review with less than 3 words\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Deleting empty rows\n",
    "data = data [data['text']!= ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the string \"[[Video ID:...]]\" from the review text\n",
    "import re\n",
    "# Initialize an empty list to store VideoID and numbers\n",
    "video_id_and_number_list = []\n",
    "\n",
    "# Define the regular expression pattern to match the [[VIDEOID:...]] text and extract the VideoID and numbers\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([^\\]]*)\\]\\]'\n",
    "\n",
    "# Extract VideoID and numbers from each row in the 'text' column and save them into the list\n",
    "for text in data['text']:\n",
    "    matches = re.findall(video_id_and_number_pattern, text)\n",
    "    for match in matches:\n",
    "        video_id_and_number_list.append('[[VIDEOID:' + match +\"]]\")\n",
    "\n",
    "# Use list to iterate through the 'text' column and replace the VideoID and numbers with an empty string\n",
    "for video_id_and_number in video_id_and_number_list:\n",
    "    data['text'] = data['text'].str.replace(video_id_and_number, '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_7686/2937091207.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace(video_id_and_number_pattern, '')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the regular expression pattern to match the entire [[VIDEOID:...]] text and any following numbers\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([0-9]+)'\n",
    "\n",
    "# Use regular expressions to remove the string \"[[VIDEOID:...]]\" and any following numbers from the review text\n",
    "data['text'] = data['text'].str.replace(video_id_and_number_pattern, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_7686/4226586295.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text_cleaned'] = data['text_cleaned'].str.replace(video_id_and_number_pattern, '')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the regular expression pattern to match the entire [[VIDEOID:...]] text and any following alphanumeric characters\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([^\\s]+)'\n",
    "\n",
    "# Use regular expressions to remove the string \"[[VIDEOID:...]]\" and any following alphanumeric characters from the review text\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(video_id_and_number_pattern, '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to preprocess text using spaCy with lemmatization and lowercasing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization and lowercasing\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc])\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding another column, that filters out stop words and punctuation/symbols\n",
    "import string\n",
    "\n",
    "# Define a set of stop words\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Function to preprocess text using spaCy\n",
    "def preprocess_text_stops(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization, lowercasing, and removal of symbols, punctuation, and stop words\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and not token.is_punct])\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the \"text\" column\n",
    "data['text_cleaned'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Apply the preprocessing function with stopword removal to the \"text\" column\n",
    "data['text_cleaned1'] = data['text'].apply(preprocess_text_stops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sentiment Analysis for Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained BERT model and tokenizer\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Function to analyze the sentiment of a text\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Perform sentiment analysis\n",
    "    outputs = sentiment_model(**inputs)\n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits)\n",
    "    return predicted_label.item()  # Return the predicted label as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the utilitarian product\n",
    "data['sentiment'] = data['text'].apply(analyze_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        5\n",
      "1        5\n",
      "2        5\n",
      "3        3\n",
      "4        5\n",
      "        ..\n",
      "22780    5\n",
      "22781    1\n",
      "22782    1\n",
      "22783    5\n",
      "22784    5\n",
      "Name: sentiment, Length: 22785, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the mapping dictionary for transforming sentiment score to match rating score 1-5, so that 0-4 gets 1-5\n",
    "sentiment_mapping = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5}\n",
    "\n",
    "# Apply the mapping to the 'sentiment' column\n",
    "data['sentiment'] = data['sentiment'].replace(sentiment_mapping)\n",
    "\n",
    "# Verify the transformation\n",
    "print(data['sentiment'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying for an different Sentiment Model (Which classifies as neutral, negative, positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['helpful_vote'] != 0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        text_cleaned Sentiment_Classification\n",
      "0  i use more experience roll but this be great f...                 positive\n",
      "1  i expect just to have some extra roll on hand ...                 positive\n",
      "2  my price line for find deal on toilet paper be...                 positive\n",
      "3  container be filthy and have huge gap expose c...                 negative\n",
      "4                                  quality and price                  neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Load the sentiment analysis model and tokenizer\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize the text into chunks of tokens with a max length of 511\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunk_size = 511\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    \n",
    "    sentiment_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Convert tokens back to text\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        # Tokenize the chunk text\n",
    "        inputs = tokenizer(chunk_text, return_tensors='pt', padding=True, truncation=True, max_length=chunk_size)\n",
    "        # Perform sentiment analysis\n",
    "        outputs = model(**inputs)\n",
    "        scores = softmax(outputs.logits[0].detach().numpy())\n",
    "        predicted_label_id = np.argmax(scores)\n",
    "        predicted_label = config.id2label[predicted_label_id]\n",
    "        sentiment_scores.append(predicted_label)\n",
    "    \n",
    "    # Aggregate the sentiment scores (e.g., by taking the majority vote)\n",
    "    return majority_vote(sentiment_scores)\n",
    "\n",
    "def majority_vote(sentiment_scores):\n",
    "    # Count the occurrences of each sentiment label\n",
    "    label_counts = {label: sentiment_scores.count(label) for label in set(sentiment_scores)}\n",
    "    # Find the label with the highest count\n",
    "    majority_label = max(label_counts, key=label_counts.get)\n",
    "    return majority_label\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Assuming df['text_cleaned'] contains the cleaned review texts\n",
    "data['Sentiment_Classification'] = data['text_cleaned'].apply(analyze_sentiment)\n",
    "\n",
    "# Display the results\n",
    "print(data[['text_cleaned', 'Sentiment_Classification']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/Final Data/Utilitarian_Final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
