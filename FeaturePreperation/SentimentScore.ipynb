{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages einlesen\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data = pd.read_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/UtiliRazor.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting special characters and review with less than 3 words\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Deleting empty rows\n",
    "data = data [data['text']!= ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of words in each row of the 'text' column\n",
    "word_counts = data['text'].str.split().str.len()\n",
    "\n",
    "# Filter rows where the word count is greater than 2\n",
    "data = data[word_counts > 3]\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "data = data[~data['text'].str.contains('é|ó', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'text' column is exactly \"Es\"\n",
    "es_pattern = r'\\bmuchas \\b'\n",
    "data = data[~data['text'].str.contains(es_pattern)]\n",
    "\n",
    "data = data[~data['text'].str.contains('este', case=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the string \"[[Video ID:...]]\" from the review text\n",
    "import re\n",
    "# Initialize an empty list to store VideoID and numbers\n",
    "video_id_and_number_list = []\n",
    "\n",
    "# Define the regular expression pattern to match the [[VIDEOID:...]] text and extract the VideoID and numbers\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([^\\]]*)\\]\\]'\n",
    "\n",
    "# Extract VideoID and numbers from each row in the 'text' column and save them into the list\n",
    "for text in data['text']:\n",
    "    matches = re.findall(video_id_and_number_pattern, text)\n",
    "    for match in matches:\n",
    "        video_id_and_number_list.append('[[VIDEOID:' + match +\"]]\")\n",
    "\n",
    "# Use list to iterate through the 'text' column and replace the VideoID and numbers with an empty string\n",
    "for video_id_and_number in video_id_and_number_list:\n",
    "    data['text'] = data['text'].str.replace(video_id_and_number, '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_4243/2937091207.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace(video_id_and_number_pattern, '')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the regular expression pattern to match the entire [[VIDEOID:...]] text and any following numbers\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([0-9]+)'\n",
    "\n",
    "# Use regular expressions to remove the string \"[[VIDEOID:...]]\" and any following numbers from the review text\n",
    "data['text'] = data['text'].str.replace(video_id_and_number_pattern, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_28106/4226586295.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text_cleaned'] = data['text_cleaned'].str.replace(video_id_and_number_pattern, '')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the regular expression pattern to match the entire [[VIDEOID:...]] text and any following alphanumeric characters\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([^\\s]+)'\n",
    "\n",
    "# Use regular expressions to remove the string \"[[VIDEOID:...]]\" and any following alphanumeric characters from the review text\n",
    "data['text_cleaned'] = data['text_cleaned'].str.replace(video_id_and_number_pattern, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/Final Data/Utilitarian_Final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to preprocess text using spaCy with lemmatization and lowercasing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization and lowercasing\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc])\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding another column, that filters out stop words and punctuation/symbols\n",
    "import string\n",
    "\n",
    "# Define a set of stop words\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Function to preprocess text using spaCy\n",
    "def preprocess_text_stops(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization, lowercasing, and removal of symbols, punctuation, and stop words\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and not token.is_punct])\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the \"text\" column\n",
    "data['text_cleaned'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Apply the preprocessing function with stopword removal to the \"text\" column\n",
    "data['text_cleaned1'] = data['text'].apply(preprocess_text_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/UtiliRazor.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sentiment Analysis for Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained BERT model and tokenizer\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Function to analyze the sentiment of a text\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Perform sentiment analysis\n",
    "    outputs = sentiment_model(**inputs)\n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits)\n",
    "    return predicted_label.item()  # Return the predicted label as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the utilitarian product\n",
    "data['sentiment'] = data['text'].apply(analyze_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        5\n",
      "1        5\n",
      "2        5\n",
      "3        3\n",
      "4        5\n",
      "        ..\n",
      "22780    5\n",
      "22781    1\n",
      "22782    1\n",
      "22783    5\n",
      "22784    5\n",
      "Name: sentiment, Length: 22785, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the mapping dictionary for transforming sentiment score to match rating score 1-5, so that 0-4 gets 1-5\n",
    "sentiment_mapping = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5}\n",
    "\n",
    "# Apply the mapping to the 'sentiment' column\n",
    "data['sentiment'] = data['sentiment'].replace(sentiment_mapping)\n",
    "\n",
    "# Verify the transformation\n",
    "print(data['sentiment'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
