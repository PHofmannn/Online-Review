{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages einlesen\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data = pd.read_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/DataPreperation/Perfume .csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting special characters and review with less than 3 words\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Deleting empty rows\n",
    "data = data [data['text']!= ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of words in each row of the 'text' column\n",
    "word_counts = data['text'].str.split().str.len()\n",
    "\n",
    "# Filter rows where the word count is greater than 2\n",
    "data = data[word_counts > 2]\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "data = data[~data['text'].str.contains('é|ó', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'text' column is exactly \"Es\"\n",
    "es_pattern = r'\\bgracias \\b'\n",
    "data = data[~data['text'].str.contains(es_pattern)]\n",
    "\n",
    "data = data[~data['text'].str.contains('biene', case=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_1934/2458081047.py:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace(video_id_and_number, '')\n",
      "/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/strings/object_array.py:156: FutureWarning: Possible nested set at position 1\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    }
   ],
   "source": [
    "# Deleting the string \"[[Video ID:...]]\" from the review text\n",
    "import re\n",
    "# Initialize an empty list to store VideoID and numbers\n",
    "video_id_and_number_list = []\n",
    "\n",
    "# Define the regular expression pattern to match the [[VIDEOID:...]] text and extract the VideoID and numbers\n",
    "video_id_and_number_pattern = r'\\[\\[VIDEOID:([^\\]]*)\\]\\]'\n",
    "\n",
    "# Extract VideoID and numbers from each row in the 'text' column and save them into the list\n",
    "for text in data['text']:\n",
    "    matches = re.findall(video_id_and_number_pattern, text)\n",
    "    for match in matches:\n",
    "        video_id_and_number_list.append('[[VIDEOID:' + match +\"]]\")\n",
    "\n",
    "# Use list to iterate through the 'text' column and replace the VideoID and numbers with an empty string\n",
    "for video_id_and_number in video_id_and_number_list:\n",
    "    data['text'] = data['text'].str.replace(video_id_and_number, '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have your data loaded into a DataFrame called df\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Drop rows where parent_asin equals \"B00WXP607C\"\n",
    "data = data[data['parent_asin'] != 'B00WXP607C']\n",
    "\n",
    "# Now df_filtered contains the rows where parent_asin is not \"B00WXP607C\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to preprocess text using spaCy with lemmatization and lowercasing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization and lowercasing\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc])\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding another column, that filters out stop words and punctuation/symbols\n",
    "import string\n",
    "\n",
    "# Define a set of stop words\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Function to preprocess text using spaCy\n",
    "def preprocess_text_stops(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatization, lowercasing, and removal of symbols, punctuation, and stop words\n",
    "    processed_text = ' '.join([token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and not token.is_punct])\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Total_Helpful_Votes  Total_Reviews\n",
      "parent_asin                                    \n",
      "B07G7FF2WC                   317            128\n",
      "B07Z8FSKG4                  3555           2761\n",
      "B0BJMV1QTR                  6399           8217\n"
     ]
    }
   ],
   "source": [
    "# Group the data by Parent ASIN and calculate the total number of helpful votes and the total number of reviews\n",
    "summary = data.groupby('parent_asin').agg(\n",
    "    Total_Helpful_Votes=('helpful_vote', 'sum'),  # Total number of helpful votes\n",
    "    Total_Reviews=('parent_asin', 'size')  # Total number of reviews\n",
    ")\n",
    "\n",
    "print (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the \"text\" column\n",
    "data['text_cleaned'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Apply the preprocessing function with stopword removal to the \"text\" column\n",
    "data['text_cleaned1'] = data['text'].apply(preprocess_text_stops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sentiment Analysis for Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained BERT model and tokenizer\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Function to analyze the sentiment of a text\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Perform sentiment analysis\n",
    "    outputs = sentiment_model(**inputs)\n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits)\n",
    "    return predicted_label.item()  # Return the predicted label as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the utilitarian product\n",
    "data['sentiment'] = data['text'].apply(analyze_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Users/paulahofmann/Documents/Coding/Online-Review/SelectingData/PerfumeNewSenti.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
