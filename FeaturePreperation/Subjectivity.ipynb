{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivitiy \n",
    "# Packages einlesen\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hedonic = pd.read_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/TestData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      rating                                            title_x  \\\n",
      "1338     5.0                                       Great value!   \n",
      "1339     5.0                            The smell is phenomenal   \n",
      "1340     1.0                                              Falso   \n",
      "1341     1.0                           Smells mostly of alcohol   \n",
      "1342     4.0                                        Smells good   \n",
      "1343     5.0                               My preferred cologne   \n",
      "1344     5.0                      Perfect summertime fragrance!   \n",
      "1345     5.0                     Great scent, medium lonngevity   \n",
      "1346     5.0  Best smelling cologne for men. Delivered in go...   \n",
      "1347     1.0  It's not OK! I can detect a fragrance weakness...   \n",
      "1348     5.0                                 OG fresh and clean   \n",
      "1349     5.0                                         Five Stars   \n",
      "1350     5.0                Such a good deal. An smells awesome   \n",
      "1351     5.0     100% real. 110% better than whatever you wear.   \n",
      "1352     4.0                                            It's ok   \n",
      "1353     5.0                                         Five Stars   \n",
      "1354     5.0                                         Way to big   \n",
      "1355     5.0                                              Best!   \n",
      "1356     2.0                                    expected bettor   \n",
      "1357     5.0             Love the way this smells on my husband   \n",
      "1358     1.0                                    Stopped working   \n",
      "1359     5.0  ... original the smell is correct and is stron...   \n",
      "1360     1.0                                      Not authentic   \n",
      "1361     4.0                                            love it   \n",
      "1362     5.0                         Best cologne for the money   \n",
      "1363     5.0  All time favorite. Like it better than Polo Bl...   \n",
      "1364     5.0                                            buy it!   \n",
      "1365     3.0                       Not the same as the original   \n",
      "1366     1.0                                       Quickly gone   \n",
      "1367     5.0                                         Five Stars   \n",
      "1368     5.0                                     WHAT a DEAL!!!   \n",
      "1369     1.0                       Buy the other blue Versace ü§ó   \n",
      "1370     5.0                                         Five Stars   \n",
      "1371     5.0            This is in the top 10 colognes for men.   \n",
      "1372     5.0                                         Five Stars   \n",
      "1373     5.0                        Smell like David Hasselhoff   \n",
      "1374     5.0                 10 out of 10! You won't regret it!   \n",
      "1375     1.0  Not sure if it's a fake or not but Im not very...   \n",
      "1376     5.0                                         Five Stars   \n",
      "1377     5.0                           Great smell long lasting   \n",
      "1378     5.0         EVERY MAN NEEDS THIS IN THEIR COLLECTION‚ö†Ô∏è   \n",
      "1379     5.0  Came in quick packaged really good I was happy...   \n",
      "1380     5.0                                        Awesome buy   \n",
      "1381     1.0                           Product is not authentic   \n",
      "1382     2.0  Does not smell like the original that I recall...   \n",
      "1383     5.0                                      The Real Deal   \n",
      "1384     1.0  If you want cheap cologne that smells like rub...   \n",
      "1385     1.0  This is my brand of cologne could fool those w...   \n",
      "\n",
      "                                                   text  \\\n",
      "1338  This is my husband's favorite scent and I got ...   \n",
      "1339  The fragrance is so attractive... little secre...   \n",
      "1340  El perfume es falso , no dura nada aplicado en...   \n",
      "1341  The scent is lovely once the alcohol dries and...   \n",
      "1342                 Smells good but does not last long   \n",
      "1343  This is my preferred sent and a repurchase.  I...   \n",
      "1344  This scent has nothing to prove! It‚Äôs my favor...   \n",
      "1345  The scent is very good, not too sweet - just p...   \n",
      "1346                           Used it for personal use   \n",
      "1347  It's not OK! I can detect a fragrance weakness...   \n",
      "1348  I wake up, spash it on and the b itches go wil...   \n",
      "1349                             Will be buying more...   \n",
      "1350  Came quickly. Not a knock off product. Such a ...   \n",
      "1351  This is real, I've been wearing this scent for...   \n",
      "1352                                            It's ok   \n",
      "1353                           This scent is awesome!!!   \n",
      "1354  A nice clean fresh smell . No bite. Subtle.<br...   \n",
      "1355          Best smelling cologne for men hands-down.   \n",
      "1356  not what I thought it was the scent was very l...   \n",
      "1357  Definitely a great buy for how big of a bottle...   \n",
      "1358  Cologne smells great but the bottle stopped sp...   \n",
      "1359  Was not a knock off it was original the smell ...   \n",
      "1360  This is not an authentic product. It does not ...   \n",
      "1361  incredible scent. kind of faint after a couple...   \n",
      "1362  I can‚Äôt tell you how many compliments I get ov...   \n",
      "1363                    Fantastic. Great gift for guys.   \n",
      "1364  Very good smell. Lasts all day for me and the ...   \n",
      "1365  I used to have this cologne a few years ago an...   \n",
      "1366  I always buy this perfume on stores and never ...   \n",
      "1367  Best Ever. It's everything that a man could want!   \n",
      "1368  AWESOME PRICE for an AWESOME product!!! I use ...   \n",
      "1369  I will never purchase this junk again first of...   \n",
      "1370                                    Very good value   \n",
      "1371  This has to be one of the best smelling cologn...   \n",
      "1372             Bottle was much larger than expected!!   \n",
      "1373  [[VIDEOID:2cae5cb69e3d1929b53f6e4a08b900d7]] c...   \n",
      "1374  If you're a guy, just buy it.  Believe me your...   \n",
      "1375  Hello all,<br />I have being using Versace Eau...   \n",
      "1376                                         Nice smell   \n",
      "1377  This cologne and size literally lasts me an en...   \n",
      "1378  1 of My Favorite \"go-to\" colognes...<br /><br ...   \n",
      "1379  Came in quick packaged really good I was happy...   \n",
      "1380  My favorite fragrance, nice amount for a great...   \n",
      "1381  I bought this product a little worried because...   \n",
      "1382  Does  not smell like the original that I recal...   \n",
      "1383  This is by far my favorite cologne for spring ...   \n",
      "1384  This stuff is fake!!!!!<br /><br />If you want...   \n",
      "1385  I bought a 1oz bottle in store, this is a very...   \n",
      "\n",
      "                                                 images        asin  \\\n",
      "1338                                                 []  B00HOZ0FGI   \n",
      "1339                                                 []  B00HOZ0FGI   \n",
      "1340                                                 []  B00HOZ0FGI   \n",
      "1341                                                 []  B00HOZ0FGI   \n",
      "1342                                                 []  B00HOZ0FGI   \n",
      "1343                                                 []  B00HOZ0FGI   \n",
      "1344                                                 []  B00HOZ0FGI   \n",
      "1345                                                 []  B00HOZ0FGI   \n",
      "1346                                                 []  B00HOZ0FGI   \n",
      "1347                                                 []  B00HOZ0FGI   \n",
      "1348                                                 []  B00HOZ0FGI   \n",
      "1349                                                 []  B00HOZ0FGI   \n",
      "1350                                                 []  B00HOZ0FGI   \n",
      "1351                                                 []  B004TMO1IM   \n",
      "1352                                                 []  B00HOZ0FGI   \n",
      "1353                                                 []  B0050WUQ8O   \n",
      "1354                                                 []  B00HOZ0FGI   \n",
      "1355                                                 []  B00HOZ0FGI   \n",
      "1356                                                 []  B00CFD25YO   \n",
      "1357                                                 []  B00HOZ0FGI   \n",
      "1358                                                 []  B00HOZ0FGI   \n",
      "1359                                                 []  B0050WUQ8O   \n",
      "1360                                                 []  B0050WUQ8O   \n",
      "1361                                                 []  B0050WUQ8O   \n",
      "1362                                                 []  B00HOZ0FGI   \n",
      "1363                                                 []  B00HOZ0FGI   \n",
      "1364                                                 []  B00HOZ0FGI   \n",
      "1365                                                 []  B0050WUQ8O   \n",
      "1366                                                 []  B00HOZ0FGI   \n",
      "1367                                                 []  B00HOZ0FGI   \n",
      "1368                                                 []  B0050WUQ8O   \n",
      "1369                                                 []  B00HOZ0FGI   \n",
      "1370                                                 []  B00HOZ0FGI   \n",
      "1371                                                 []  B00HOZ0FGI   \n",
      "1372                                                 []  B0050WUQ8O   \n",
      "1373                                                 []  B00HOZ0FGI   \n",
      "1374                                                 []  B00HOZ0FGI   \n",
      "1375                                                 []  B00HOZ0FGI   \n",
      "1376                                                 []  B00HOZ0FGI   \n",
      "1377                                                 []  B00HOZ0FGI   \n",
      "1378                                                 []  B00HOZ0FGI   \n",
      "1379                                                 []  B00HOZ0FGI   \n",
      "1380                                                 []  B00HOZ0FGI   \n",
      "1381                                                 []  B00HOZ0FGI   \n",
      "1382                                                 []  B00HOZ0FGI   \n",
      "1383  [{'small_image_url': 'https://m.media-amazon.c...  B00HOZ0FGI   \n",
      "1384                                                 []  B0050WUQ8O   \n",
      "1385  [{'small_image_url': 'https://images-na.ssl-im...  B0050WUQ8O   \n",
      "\n",
      "     parent_asin                       user_id            timestamp  \\\n",
      "1338  B00WXP607C  AHD77DK23ZNYG2S342MLYCBXVGVQ  2020-10-15 21:46:21   \n",
      "1339  B00WXP607C  AFTWMLW2RZRAZD2LODTRVGJTZZ7Q  2021-07-28 13:40:02   \n",
      "1340  B00WXP607C  AHWAA5KA5GID3M5A5NOKD7PPWXOQ  2021-08-05 22:13:56   \n",
      "1341  B00WXP607C  AH5HKVWCVUUGGFA2VON2NUFOZZWA  2018-12-20 03:16:56   \n",
      "1342  B00WXP607C  AFAZIG3HVPYD26Y2G3SA3SMYLTYQ  2022-06-24 20:32:45   \n",
      "1343  B00WXP607C  AEJTAAJQQGIT57MPDSN3V2D2JPVA  2021-03-08 02:59:03   \n",
      "1344  B00WXP607C  AG4T4MXNVTJWQY5TUYMSOCVSROWQ  2022-06-03 01:20:31   \n",
      "1345  B00WXP607C  AH4MZCUTDMTZ6H5AEG2DOZY34QJQ  2022-05-26 18:57:33   \n",
      "1346  B00WXP607C  AGHPV63YJY56RT2SYTEOHBN6DZ2Q  2022-06-15 19:08:03   \n",
      "1347  B00WXP607C  AESKYT4Z5XDBI4XRGBY2YBGDG5OA  2017-04-03 18:57:08   \n",
      "1348  B00WXP607C  AGUJDKQMGHQQR4RIZ5FOE5MOQBFQ  2021-08-07 18:09:09   \n",
      "1349  B00WXP607C  AESFKNURS4PE26ML2L77UQGYK4KA  2016-10-21 18:33:54   \n",
      "1350  B00WXP607C  AGZXMGWRHQBP6BDNUQMKE2VMIJOQ  2018-06-18 13:08:43   \n",
      "1351  B00WXP607C  AGAA6KSKXNTVZD5IRVDCTJMSXMNQ  2022-01-15 01:27:54   \n",
      "1352  B00WXP607C  AFXK5B3MVRTHGYMMITXSN5UBXSEA  2022-10-25 19:34:41   \n",
      "1353  B00WXP607C  AG6CX473SKG6PME5LGM6QB3FWYUQ  2017-12-20 14:09:06   \n",
      "1354  B00WXP607C  AFIJVQVDBNVIMPAARXRTBY5HVXZQ  2016-09-12 22:54:32   \n",
      "1355  B00WXP607C  AEKZTW7TUBKUS65YSW7BEZ7TB5PQ  2022-01-20 10:53:11   \n",
      "1356  B00WXP607C  AE545TDM7DFSRTG7OVQX3F4R5M3A  2014-04-02 23:00:47   \n",
      "1357  B00WXP607C  AEYYAO4OP2QR4SUTGXMS3R56SLLA  2022-10-30 09:57:50   \n",
      "1358  B00WXP607C  AGADJUHLS2GDRUYSHG4AVXFLVKNQ  2022-04-12 00:57:02   \n",
      "1359  B00WXP607C  AFPAZIOMSDEEFMRNCNJIR6WTTRVA  2017-10-24 20:47:07   \n",
      "1360  B00WXP607C  AEZJHYTUP6MLY7ASS3XIGVCLW3JQ  2018-01-04 02:14:26   \n",
      "1361  B00WXP607C  AGHAWZQ57KJCX3VBGETQMYUSYTTA  2016-07-29 02:17:00   \n",
      "1362  B00WXP607C  AGDXINBCYZVPBEBT5SZ2ROFCAICQ  2022-04-21 23:12:56   \n",
      "1363  B00WXP607C  AGTQB2S35YFQSWMAVVD7GL2T7UFA  2016-09-29 23:58:25   \n",
      "1364  B00WXP607C  AGYP4TAA4LLX3Q5JG2HZQAMX2ZZQ  2017-02-28 15:05:51   \n",
      "1365  B00WXP607C  AEFT72AWIYRCPHGRZTHGNPS3JDGQ  2018-01-10 19:14:49   \n",
      "1366  B00WXP607C  AHD3WR4OJQSSDAFGCXFBAOG6BENA  2022-05-17 05:57:06   \n",
      "1367  B00WXP607C  AHSLTZ3RZ3W7TP3MTZMHGHRDXNHA  2016-06-16 14:41:51   \n",
      "1368  B00WXP607C  AGPEV4J4HUKOBGQA725PYLLUXSWQ  2017-02-08 14:59:49   \n",
      "1369  B00WXP607C  AFRSYAM6HLPGVDEZ7HVKPQPLEWWA  2019-08-14 05:09:42   \n",
      "1370  B00WXP607C  AGAYB3MDYRFZZVRZV5GDDJFZYXDA  2016-02-14 01:16:59   \n",
      "1371  B00WXP607C  AF2DELO4K5VUVFVXBRGDSQEL43HA  2021-12-22 15:58:34   \n",
      "1372  B00WXP607C  AGHZKHALTH2CNBN2JGVK7DZBW5TA  2018-02-04 01:19:11   \n",
      "1373  B00WXP607C  AGVSDTG4UENWO5IYNUDIULGYC4AA  2023-01-05 16:49:00   \n",
      "1374  B00WXP607C  AFNBTQFS3QI7GPAQJWTRH4LG3BGA  2022-06-27 01:34:44   \n",
      "1375  B00WXP607C  AEJSGZP7UVGLW4KREBOFICA6527Q  2017-09-17 22:43:56   \n",
      "1376  B00WXP607C  AF4R4SWST6YVGS26ZO2ZXN7ECEMQ  2016-09-08 00:53:15   \n",
      "1377  B00WXP607C  AEUSPLHRXBA5C75F2A542OMLPH6A  2022-07-15 03:04:57   \n",
      "1378  B00WXP607C  AG2WLCJJCFEILPB2Q2Z3S4MR6WAA  2018-06-23 17:50:18   \n",
      "1379  B00WXP607C  AGY52LTTX2ZOJ6HYLQWNXPH7W44Q  2018-04-10 07:22:37   \n",
      "1380  B00WXP607C  AELY2XJ66ELAQIQ2VY5FL4NJESBQ  2016-12-02 18:09:16   \n",
      "1381  B00WXP607C  AE3QACFRE6DSYAN2BDF2SSYPQKRQ  2018-09-04 17:32:06   \n",
      "1382  B00WXP607C  AEP6IKXHUY5GH5ABCASZTJN43WPA  2017-11-15 00:18:56   \n",
      "1383  B00WXP607C  AGCAHCOZC4DK36B73W4SCZMU4B7Q  2022-06-29 18:35:31   \n",
      "1384  B00WXP607C  AEZ5OBYRWHPHFGOPV7R6OBUBVH5Q  2017-02-05 23:58:11   \n",
      "1385  B00WXP607C  AFVTFOQG7UN7N55FVJVJ2QCK2LNQ  2017-09-23 18:41:43   \n",
      "\n",
      "      helpful_vote  verified_purchase  ...      product ver_purch    #nouns  \\\n",
      "1338             2               True  ...  Men Cologne         1  0.200000   \n",
      "1339             1               True  ...  Men Cologne         1  0.173913   \n",
      "1340             1               True  ...  Men Cologne         1  0.333333   \n",
      "1341             3               True  ...  Men Cologne         1  0.161290   \n",
      "1342             1               True  ...  Men Cologne         1  0.000000   \n",
      "1343             1               True  ...  Men Cologne         1  0.066667   \n",
      "1344             1               True  ...  Men Cologne         1  0.093750   \n",
      "1345             1               True  ...  Men Cologne         1  0.187500   \n",
      "1346             1               True  ...  Men Cologne         1  0.200000   \n",
      "1347             7               True  ...  Men Cologne         1  0.240000   \n",
      "1348             1               True  ...  Men Cologne         1  0.142857   \n",
      "1349             1               True  ...  Men Cologne         1  0.000000   \n",
      "1350             1               True  ...  Men Cologne         1  0.157895   \n",
      "1351             1               True  ...  Men Cologne         1  0.175439   \n",
      "1352             2               True  ...  Men Cologne         1  0.000000   \n",
      "1353             2               True  ...  Men Cologne         1  0.250000   \n",
      "1354             1               True  ...  Men Cologne         1  0.352941   \n",
      "1355             2               True  ...  Men Cologne         1  0.500000   \n",
      "1356             2               True  ...  Men Cologne         1  0.090909   \n",
      "1357             1               True  ...  Men Cologne         1  0.166667   \n",
      "1358             1               True  ...  Men Cologne         1  0.200000   \n",
      "1359             1               True  ...  Men Cologne         1  0.136364   \n",
      "1360             2               True  ...  Men Cologne         1  0.181818   \n",
      "1361             2               True  ...  Men Cologne         1  0.333333   \n",
      "1362             1               True  ...  Men Cologne         1  0.125000   \n",
      "1363             2               True  ...  Men Cologne         1  0.400000   \n",
      "1364             1               True  ...  Men Cologne         1  0.222222   \n",
      "1365            10               True  ...  Men Cologne         1  0.150000   \n",
      "1366             1               True  ...  Men Cologne         1  0.233333   \n",
      "1367             1               True  ...  Men Cologne         1  0.111111   \n",
      "1368             6               True  ...  Men Cologne         1  0.200000   \n",
      "1369             5               True  ...  Men Cologne         1  0.160494   \n",
      "1370             1               True  ...  Men Cologne         1  0.333333   \n",
      "1371            11               True  ...  Men Cologne         1  0.204082   \n",
      "1372             1               True  ...  Men Cologne         1  0.000000   \n",
      "1373            11               True  ...  Men Cologne         1  0.166667   \n",
      "1374             4               True  ...  Men Cologne         1  0.074627   \n",
      "1375            28               True  ...  Men Cologne         1  0.159091   \n",
      "1376             1               True  ...  Men Cologne         1  0.500000   \n",
      "1377             1               True  ...  Men Cologne         1  0.173077   \n",
      "1378             5               True  ...  Men Cologne         1  0.119048   \n",
      "1379             1               True  ...  Men Cologne         1  0.125000   \n",
      "1380             1               True  ...  Men Cologne         1  0.333333   \n",
      "1381            13               True  ...  Men Cologne         1  0.187500   \n",
      "1382             2               True  ...  Men Cologne         1  0.058824   \n",
      "1383             2               True  ...  Men Cologne         1  0.188034   \n",
      "1384             2              False  ...  Men Cologne         0  0.238095   \n",
      "1385            10               True  ...  Men Cologne         1  0.234568   \n",
      "\n",
      "          #adj      #adv  product_type  product_n prod_cat  \\\n",
      "1338  0.085714  0.085714             1          4        1   \n",
      "1339  0.173913  0.043478             1          4        1   \n",
      "1340  0.083333  0.000000             1          4        1   \n",
      "1341  0.112903  0.064516             1          4        1   \n",
      "1342  0.142857  0.142857             1          4        1   \n",
      "1343  0.066667  0.066667             1          4        1   \n",
      "1344  0.093750  0.046875             1          4        1   \n",
      "1345  0.250000  0.187500             1          4        1   \n",
      "1346  0.200000  0.000000             1          4        1   \n",
      "1347  0.040000  0.040000             1          4        1   \n",
      "1348  0.142857  0.000000             1          4        1   \n",
      "1349  0.250000  0.000000             1          4        1   \n",
      "1350  0.105263  0.052632             1          4        1   \n",
      "1351  0.140351  0.070175             1          4        1   \n",
      "1352  0.500000  0.000000             1          4        1   \n",
      "1353  0.250000  0.000000             1          4        1   \n",
      "1354  0.235294  0.000000             1          4        1   \n",
      "1355  0.166667  0.000000             1          4        1   \n",
      "1356  0.136364  0.045455             1          4        1   \n",
      "1357  0.166667  0.083333             1          4        1   \n",
      "1358  0.100000  0.050000             1          4        1   \n",
      "1359  0.181818  0.045455             1          4        1   \n",
      "1360  0.090909  0.121212             1          4        1   \n",
      "1361  0.111111  0.055556             1          4        1   \n",
      "1362  0.062500  0.000000             1          4        1   \n",
      "1363  0.400000  0.000000             1          4        1   \n",
      "1364  0.111111  0.055556             1          4        1   \n",
      "1365  0.050000  0.100000             1          4        1   \n",
      "1366  0.033333  0.133333             1          4        1   \n",
      "1367  0.000000  0.222222             1          4        1   \n",
      "1368  0.080000  0.000000             1          4        1   \n",
      "1369  0.098765  0.098765             1          4        1   \n",
      "1370  0.333333  0.333333             1          4        1   \n",
      "1371  0.102041  0.000000             1          4        1   \n",
      "1372  0.166667  0.166667             1          4        1   \n",
      "1373  0.000000  0.000000             1          4        1   \n",
      "1374  0.059701  0.059701             1          4        1   \n",
      "1375  0.079545  0.090909             1          4        1   \n",
      "1376  0.500000  0.000000             1          4        1   \n",
      "1377  0.115385  0.076923             1          4        1   \n",
      "1378  0.047619  0.071429             1          4        1   \n",
      "1379  0.187500  0.031250             1          4        1   \n",
      "1380  0.333333  0.000000             1          4        1   \n",
      "1381  0.093750  0.156250             1          4        1   \n",
      "1382  0.000000  0.000000             1          4        1   \n",
      "1383  0.119658  0.085470             1          4        1   \n",
      "1384  0.142857  0.000000             1          4        1   \n",
      "1385  0.074074  0.037037             1          4        1   \n",
      "\n",
      "                     labels                                      scores  \n",
      "1338  [SUBJECTIVE, NEUTRAL]  [0.9756090044975281, 0.024391015991568565]  \n",
      "1339  [SUBJECTIVE, NEUTRAL]   [0.8884975910186768, 0.11150244623422623]  \n",
      "1340  [NEUTRAL, SUBJECTIVE]   [0.8408468961715698, 0.15915314853191376]  \n",
      "1341  [SUBJECTIVE, NEUTRAL]   [0.8919496536254883, 0.10805036872625351]  \n",
      "1342  [SUBJECTIVE, NEUTRAL]    [0.5637558698654175, 0.4362441897392273]  \n",
      "1343  [SUBJECTIVE, NEUTRAL]    [0.7428399920463562, 0.2571600675582886]  \n",
      "1344  [SUBJECTIVE, NEUTRAL]   [0.9529253840446472, 0.04707461595535278]  \n",
      "1345  [SUBJECTIVE, NEUTRAL]   [0.8945615887641907, 0.10543843358755112]  \n",
      "1346  [NEUTRAL, SUBJECTIVE]    [0.876065194606781, 0.12393482774496078]  \n",
      "1347  [SUBJECTIVE, NEUTRAL]    [0.6960124373435974, 0.3039875328540802]  \n",
      "1348  [SUBJECTIVE, NEUTRAL]   [0.8774853348731995, 0.12251468002796173]  \n",
      "1349  [SUBJECTIVE, NEUTRAL]    [0.6078492403030396, 0.3921507000923157]  \n",
      "1350  [SUBJECTIVE, NEUTRAL]   [0.8716225624084473, 0.12837746739387512]  \n",
      "1351  [SUBJECTIVE, NEUTRAL]   [0.9094383120536804, 0.09056171774864197]  \n",
      "1352  [SUBJECTIVE, NEUTRAL]    [0.7631625533103943, 0.2368374913930893]  \n",
      "1353  [SUBJECTIVE, NEUTRAL]  [0.9866674542427063, 0.013332623988389969]  \n",
      "1354  [SUBJECTIVE, NEUTRAL]   [0.7543362379074097, 0.24566377699375153]  \n",
      "1355  [SUBJECTIVE, NEUTRAL]  [0.9947224855422974, 0.005277502816170454]  \n",
      "1356  [SUBJECTIVE, NEUTRAL]     [0.567069411277771, 0.4329306185245514]  \n",
      "1357  [SUBJECTIVE, NEUTRAL]  [0.9648725986480713, 0.035127393901348114]  \n",
      "1358  [SUBJECTIVE, NEUTRAL]   [0.9047197103500366, 0.09528030455112457]  \n",
      "1359  [NEUTRAL, SUBJECTIVE]    [0.530972957611084, 0.46902701258659363]  \n",
      "1360  [SUBJECTIVE, NEUTRAL]   [0.7011138200759888, 0.29888617992401123]  \n",
      "1361  [SUBJECTIVE, NEUTRAL]  [0.9581064581871033, 0.041893571615219116]  \n",
      "1362  [SUBJECTIVE, NEUTRAL]    [0.929919421672821, 0.07008057087659836]  \n",
      "1363  [SUBJECTIVE, NEUTRAL]   [0.992435872554779, 0.007564172148704529]  \n",
      "1364  [SUBJECTIVE, NEUTRAL]   [0.8150507211685181, 0.18494929373264313]  \n",
      "1365  [SUBJECTIVE, NEUTRAL]   [0.9639686942100525, 0.03603128716349602]  \n",
      "1366  [SUBJECTIVE, NEUTRAL]  [0.9685922861099243, 0.031407661736011505]  \n",
      "1367  [SUBJECTIVE, NEUTRAL]   [0.9645227789878845, 0.03547726199030876]  \n",
      "1368  [SUBJECTIVE, NEUTRAL]  [0.9849616289138794, 0.015038369223475456]  \n",
      "1369  [SUBJECTIVE, NEUTRAL]    [0.926773726940155, 0.07322625815868378]  \n",
      "1370  [SUBJECTIVE, NEUTRAL]   [0.8902270793914795, 0.10977289825677872]  \n",
      "1371  [SUBJECTIVE, NEUTRAL]  [0.9750271439552307, 0.024972811341285706]  \n",
      "1372  [SUBJECTIVE, NEUTRAL]   [0.8528375029563904, 0.14716249704360962]  \n",
      "1373  [SUBJECTIVE, NEUTRAL]   [0.9231984615325928, 0.07680150866508484]  \n",
      "1374  [SUBJECTIVE, NEUTRAL]    [0.798265814781189, 0.20173422992229462]  \n",
      "1375  [NEUTRAL, SUBJECTIVE]    [0.5210959911346436, 0.4789040684700012]  \n",
      "1376  [SUBJECTIVE, NEUTRAL]   [0.9558649659156799, 0.04413506016135216]  \n",
      "1377  [NEUTRAL, SUBJECTIVE]    [0.6096612811088562, 0.3903387188911438]  \n",
      "1378  [SUBJECTIVE, NEUTRAL]  [0.9918767213821411, 0.008123256266117096]  \n",
      "1379  [SUBJECTIVE, NEUTRAL]    [0.8045836091041565, 0.1954164206981659]  \n",
      "1380  [SUBJECTIVE, NEUTRAL]   [0.9790605306625366, 0.02093946933746338]  \n",
      "1381  [SUBJECTIVE, NEUTRAL]    [0.7355201244354248, 0.2644798457622528]  \n",
      "1382  [SUBJECTIVE, NEUTRAL]   [0.5303484201431274, 0.46965157985687256]  \n",
      "1383  [SUBJECTIVE, NEUTRAL]    [0.8294519782066345, 0.1705479919910431]  \n",
      "1384  [SUBJECTIVE, NEUTRAL]   [0.7477848529815674, 0.25221511721611023]  \n",
      "1385  [SUBJECTIVE, NEUTRAL]   [0.8265554308891296, 0.17344455420970917]  \n",
      "\n",
      "[48 rows x 45 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_61578/1693662920.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['classification_scores'] = filtered_df['text_cleaned'].apply(classify_text)\n",
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_61578/1693662920.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['labels'] = labels_list\n",
      "/var/folders/ss/4y6dgpw950348n7xdxy73vnm0000gn/T/ipykernel_61578/1693662920.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['scores'] = scores_list\n",
      "/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the text classification pipeline\n",
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    top_k=None,  # Equivalent to return_all_scores=True\n",
    ")\n",
    "\n",
    "# Function to classify review text and return the scores for all labels\n",
    "def classify_text(text):\n",
    "    classification_result = classify(text)\n",
    "    return classification_result[0]\n",
    "\n",
    "\n",
    "\n",
    "# Apply text classification to each review text in the DataFrame and extract scores for all labels\n",
    "filtered_df['classification_scores'] = filtered_df['text_cleaned'].apply(classify_text)\n",
    "\n",
    "# Extract label and score from each dictionary in the list\n",
    "labels_list = []\n",
    "scores_list = []\n",
    "for i, row in filtered_df.iterrows():\n",
    "    label_scores = row['classification_scores']\n",
    "    labels = [entry['label'] for entry in label_scores]\n",
    "    scores = [entry['score'] for entry in label_scores]\n",
    "    labels_list.append(labels)\n",
    "    scores_list.append(scores)\n",
    "\n",
    "# Assign extracted labels and scores to new columns in the DataFrame\n",
    "filtered_df['labels'] = labels_list\n",
    "filtered_df['scores'] = scores_list\n",
    "\n",
    "# Drop the 'classification_scores' column as it's no longer needed\n",
    "filtered_df.drop(columns=['classification_scores'], inplace=True)\n",
    "\n",
    "print(filtered_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1146) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m classification_result[\u001b[39m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[39m# Apply text classification to each review text in the DataFrame and extract scores for all labels\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m data_hedonic[\u001b[39m'\u001b[39m\u001b[39mclassification_scores\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data_hedonic[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(classify_text)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Extract subjective and neutral scores separately\u001b[39;00m\n\u001b[1;32m     21\u001b[0m subjective_scores \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m   4254\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4355\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4357\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1093\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1099\u001b[0m             values,\n\u001b[1;32m   1100\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1101\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1102\u001b[0m         )\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1105\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mclassify_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_text\u001b[39m(text):\n\u001b[0;32m---> 13\u001b[0m     classification_result \u001b[39m=\u001b[39m classify(text)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m classification_result[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,)\n\u001b[0;32m--> 156\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1199\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1200\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         )\n\u001b[1;32m   1204\u001b[0m     )\n\u001b[1;32m   1205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1212\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1213\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1214\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1565\u001b[0m     input_ids,\n\u001b[1;32m   1566\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1567\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1568\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1569\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1570\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1574\u001b[0m )\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1009\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1010\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[1;32m   1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 238\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[1;32m    239\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    240\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1146) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the text classification pipeline\n",
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    top_k=None,  # Equivalent to return_all_scores=True\n",
    ")\n",
    "\n",
    "# Function to classify review text and return the scores for all labels\n",
    "def classify_text(text):\n",
    "    classification_result = classify(text)\n",
    "    return classification_result[0]\n",
    "\n",
    "\n",
    "# Apply text classification to each review text in the DataFrame and extract scores for all labels\n",
    "data_hedonic['classification_scores'] = data_hedonic['text'].apply(classify_text)\n",
    "\n",
    "# Extract subjective and neutral scores separately\n",
    "subjective_scores = []\n",
    "neutral_scores = []\n",
    "for _, row in data_hedonic.iterrows():\n",
    "    label_scores = row['classification_scores']\n",
    "    subjective_score = next((entry['score'] for entry in label_scores if entry['label'] == 'SUBJECTIVE'), None)\n",
    "    neutral_score = next((entry['score'] for entry in label_scores if entry['label'] == 'NEUTRAL'), None)\n",
    "    subjective_scores.append(subjective_score)\n",
    "    neutral_scores.append(neutral_score)\n",
    "\n",
    "# Add the extracted scores as new columns to the DataFrame\n",
    "data_hedonic['subjective_score'] = subjective_scores\n",
    "data_hedonic['neutral_score'] = neutral_scores\n",
    "\n",
    "# Drop the 'classification_scores' column as it's no longer needed\n",
    "data_hedonic.drop(columns=['classification_scores'], inplace=True)\n",
    "\n",
    "print(data_hedonic)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
