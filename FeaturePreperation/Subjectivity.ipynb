{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivitiy \n",
    "# Packages einlesen\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hedonic = pd.read_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/TestCasual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hedonic = data_hedonic[data_hedonic['product'] == \"Gaming Mouse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m      4\u001b[0m \u001b[39m# Define the text classification pipeline\u001b[39;00m\n\u001b[1;32m      5\u001b[0m classify \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m      6\u001b[0m     task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-classification\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcffl/bert-base-styleclassification-subjective-neutral\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# Equivalent to return_all_scores=True\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/utils/import_utils.py:1462\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1460\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1461\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1462\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1463\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/utils/import_utils.py:1472\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1471\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1472\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1473\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1474\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1475\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1476\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1477\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/__init__.py:63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconversational\u001b[39;00m \u001b[39mimport\u001b[39;00m Conversation, ConversationalPipeline\n\u001b[1;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdepth_estimation\u001b[39;00m \u001b[39mimport\u001b[39;00m DepthEstimationPipeline\n\u001b[0;32m---> 63\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdocument_question_answering\u001b[39;00m \u001b[39mimport\u001b[39;00m DocumentQuestionAnsweringPipeline\n\u001b[1;32m     64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureExtractionPipeline\n\u001b[1;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfill_mask\u001b[39;00m \u001b[39mimport\u001b[39;00m FillMaskPipeline\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/document_question_answering.py:29\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     ExplicitEnum,\n\u001b[1;32m     22\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     logging,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ChunkPipeline, build_pipeline_init_args\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquestion_answering\u001b[39;00m \u001b[39mimport\u001b[39;00m select_starts_ends\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m is_vision_available():\n\u001b[1;32m     33\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m SquadExample, SquadFeatures, squad_convert_examples_to_features\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodelcard\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelCard\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/data/__init__.py:26\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata_collator\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m     17\u001b[0m     DataCollatorForPermutationLanguageModeling,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     default_data_collator,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mprocessors\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     DataProcessor,\n\u001b[1;32m     29\u001b[0m     InputExample,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/data/metrics/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m     19\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m pearsonr, spearmanr\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m f1_score, matthews_corrcoef\n\u001b[1;32m     23\u001b[0m DEPRECATION_WARNING \u001b[39m=\u001b[39m (\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThis metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlibrary. You can have a look at this example script for pointers: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimple_accuracy\u001b[39m(preds, labels):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/sklearn/__init__.py:83\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[1;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[1;32m     86\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    130\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/sklearn/utils/__init__.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bunch\u001b[39;00m \u001b[39mimport\u001b[39;00m Bunch\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:971\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:640\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the text classification pipeline\n",
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    top_k=None,  # Equivalent to return_all_scores=True\n",
    ")\n",
    "\n",
    "# Function to classify review text and return the scores for all labels\n",
    "def classify_text(text):\n",
    "    classification_result = classify(text)\n",
    "    return classification_result[0]\n",
    "\n",
    "\n",
    "\n",
    "# Apply text classification to each review text in the DataFrame and extract scores for all labels\n",
    "filtered_df['classification_scores'] = filtered_df['text_cleaned'].apply(classify_text)\n",
    "\n",
    "# Extract label and score from each dictionary in the list\n",
    "labels_list = []\n",
    "scores_list = []\n",
    "for i, row in filtered_df.iterrows():\n",
    "    label_scores = row['classification_scores']\n",
    "    labels = [entry['label'] for entry in label_scores]\n",
    "    scores = [entry['score'] for entry in label_scores]\n",
    "    labels_list.append(labels)\n",
    "    scores_list.append(scores)\n",
    "\n",
    "# Assign extracted labels and scores to new columns in the DataFrame\n",
    "filtered_df['labels'] = labels_list\n",
    "filtered_df['scores'] = scores_list\n",
    "\n",
    "# Drop the 'classification_scores' column as it's no longer needed\n",
    "filtered_df.drop(columns=['classification_scores'], inplace=True)\n",
    "\n",
    "print(filtered_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (857 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (857) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m classification_result[\u001b[39m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[39m# Apply text classification to each review text in the DataFrame and extract scores for all labels\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m data_hedonic[\u001b[39m'\u001b[39m\u001b[39mclassification_scores\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data_hedonic[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(classify_text)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Extract subjective and neutral scores separately\u001b[39;00m\n\u001b[1;32m     21\u001b[0m subjective_scores \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m   4254\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4355\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4357\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1093\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1099\u001b[0m             values,\n\u001b[1;32m   1100\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1101\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1102\u001b[0m         )\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1105\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mclassify_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_text\u001b[39m(text):\n\u001b[0;32m---> 13\u001b[0m     classification_result \u001b[39m=\u001b[39m classify(text)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m classification_result[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,)\n\u001b[0;32m--> 156\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1199\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1200\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         )\n\u001b[1;32m   1204\u001b[0m     )\n\u001b[1;32m   1205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1212\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1213\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1214\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1565\u001b[0m     input_ids,\n\u001b[1;32m   1566\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1567\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1568\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1569\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1570\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1574\u001b[0m )\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1009\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1010\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[1;32m   1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 238\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[1;32m    239\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    240\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (857) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the text classification pipeline\n",
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    top_k=None,  # Equivalent to return_all_scores=True\n",
    ")\n",
    "\n",
    "# Function to classify review text and return the scores for all labels\n",
    "def classify_text(text):\n",
    "    classification_result = classify(text)\n",
    "    return classification_result[0]\n",
    "\n",
    "\n",
    "# Apply text classification to each review text in the DataFrame and extract scores for all labels\n",
    "data_hedonic['classification_scores'] = data_hedonic['text'].apply(classify_text)\n",
    "\n",
    "# Extract subjective and neutral scores separately\n",
    "subjective_scores = []\n",
    "neutral_scores = []\n",
    "for _, row in data_hedonic.iterrows():\n",
    "    label_scores = row['classification_scores']\n",
    "    subjective_score = next((entry['score'] for entry in label_scores if entry['label'] == 'SUBJECTIVE'), None)\n",
    "    neutral_score = next((entry['score'] for entry in label_scores if entry['label'] == 'NEUTRAL'), None)\n",
    "    subjective_scores.append(subjective_score)\n",
    "    neutral_scores.append(neutral_score)\n",
    "\n",
    "# Add the extracted scores as new columns to the DataFrame\n",
    "data_hedonic['subjective_score'] = subjective_scores\n",
    "data_hedonic['neutral_score'] = neutral_scores\n",
    "\n",
    "# Drop the 'classification_scores' column as it's no longer needed\n",
    "data_hedonic.drop(columns=['classification_scores'], inplace=True)\n",
    "\n",
    "print(data_hedonic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      rating                                          title_x  \\\n",
      "0        5.0                                         Perfect!   \n",
      "1        5.0                                         Amazing!   \n",
      "2        5.0                                      The best!!!   \n",
      "3        5.0                                Performs Miracles   \n",
      "4        5.0                                     Great stuff!   \n",
      "...      ...                                              ...   \n",
      "6781     5.0                                    Monster Tamer   \n",
      "6782     5.0                                          Awesome   \n",
      "6783     5.0                                    Great Product   \n",
      "6784     5.0                                        Resultado   \n",
      "6785     5.0  This brush has lifted a weight off my shoulders   \n",
      "\n",
      "                                                   text  \\\n",
      "0      A lot of product for the price and itâ€™s awesome!   \n",
      "1     Made my nails strong and grow fast with regula...   \n",
      "2     I use this on my hands and nails/cuticles.  I ...   \n",
      "3     Having problem cuticles and nails due to nail ...   \n",
      "4     Highly recommend! My hands and cuticles are al...   \n",
      "...                                                 ...   \n",
      "6781  My youngest has hair down past her bottom, she...   \n",
      "6782  My daughter is mixed and has hair that always ...   \n",
      "6783  Got it for my dog. She absolutely loves being ...   \n",
      "6784            Me encanto, no daÃ±a! SÃºper espectacular   \n",
      "6785  No my toddler still did not like me brushing h...   \n",
      "\n",
      "                                                 images        asin  \\\n",
      "0                                                    []  B08HSQ5RBM   \n",
      "1     [{'small_image_url': 'https://m.media-amazon.c...  B00F644L32   \n",
      "2                                                    []  B00F644LTQ   \n",
      "3                                                    []  B00F644LTQ   \n",
      "4                                                    []  B08TQ83Z4V   \n",
      "...                                                 ...         ...   \n",
      "6781                                                 []  B084D6T768   \n",
      "6782                                                 []  B00JJ7T2V8   \n",
      "6783                                                 []  B07L61DNMR   \n",
      "6784                                                 []  B00JJ7T2V8   \n",
      "6785                                                 []  B07VHP5Y6S   \n",
      "\n",
      "     parent_asin                       user_id            timestamp  \\\n",
      "0     B09GWLJPTH  AEM3JNGXGX6U7RAZUPGTGGN2CUWA  2022-10-05 07:55:09   \n",
      "1     B09GWLJPTH  AEDQJOXMUDJ3EEXWR2QYU3UN73WQ  2022-09-01 18:50:02   \n",
      "2     B09GWLJPTH  AHOUEEGPXZY52O7RHMG5VR76IKJA  2021-01-27 04:05:44   \n",
      "3     B09GWLJPTH  AECXF5T4OZIQCPCBGLLCUULFZTXQ  2021-01-31 20:50:41   \n",
      "4     B09GWLJPTH  AET3NI3BRHN2MRJP32ZTO52KZRDQ  2022-08-22 00:28:19   \n",
      "...          ...                           ...                  ...   \n",
      "6781  B01EY96W72  AGZFY66PPV63OC7KAWZQBH6ABKKQ  2020-05-07 16:01:28   \n",
      "6782  B01EY96W72  AEZ4MZKLIZH5NJMZP4R7GF25JWCA  2023-06-18 19:55:23   \n",
      "6783  B01EY96W72  AH3T5DRGJYSKFNNGA25CO4D572LA  2023-08-06 18:53:21   \n",
      "6784  B01EY96W72  AESHTS76OFZSD7BRF4T6J4NCLUUQ  2023-06-02 17:52:42   \n",
      "6785  B01EY96W72  AH2ZV3NOTXBNF5SZUTWTOVJOMHUA  2023-07-06 01:33:15   \n",
      "\n",
      "      helpful_vote  verified_purchase  ... day hour     product ver_purch  \\\n",
      "0                1               True  ...   5    7    Face Oil         1   \n",
      "1                7               True  ...   1   18    Face Oil         1   \n",
      "2                1               True  ...  27    4    Face Oil         1   \n",
      "3                1               True  ...  31   20    Face Oil         1   \n",
      "4                1              False  ...  22    0    Face Oil         0   \n",
      "...            ...                ...  ...  ..  ...         ...       ...   \n",
      "6781             2               True  ...   7   16  Hair Brush         1   \n",
      "6782             1               True  ...  18   19  Hair Brush         1   \n",
      "6783             1               True  ...   6   18  Hair Brush         1   \n",
      "6784             1               True  ...   2   17  Hair Brush         1   \n",
      "6785             1               True  ...   6    1  Hair Brush         1   \n",
      "\n",
      "        #nouns      #adj      #adv prod_type  subjective_score  neutral_score  \n",
      "0     0.300000  0.100000  0.000000         1          0.993462       0.006538  \n",
      "1     0.200000  0.200000  0.100000         1          0.304568       0.695432  \n",
      "2     0.171548  0.083682  0.083682         1          0.939508       0.060492  \n",
      "3     0.239362  0.138298  0.058511         1          0.645694       0.354306  \n",
      "4     0.266667  0.066667  0.266667         1          0.940159       0.059840  \n",
      "...        ...       ...       ...       ...               ...            ...  \n",
      "6781  0.207547  0.075472  0.075472         0          0.927626       0.072374  \n",
      "6782  0.185185  0.037037  0.148148         0          0.906211       0.093789  \n",
      "6783  0.263158  0.052632  0.052632         0          0.943160       0.056840  \n",
      "6784  0.166667  0.166667  0.000000         0          0.913743       0.086257  \n",
      "6785  0.213115  0.032787  0.147541         0          0.994557       0.005443  \n",
      "\n",
      "[6786 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the text classification pipeline\n",
    "classify = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"cffl/bert-base-styleclassification-subjective-neutral\",\n",
    "    top_k=None,  # Equivalent to return_all_scores=True\n",
    ")\n",
    "\n",
    "# Function to classify review text and return the scores for all labels\n",
    "def classify_text(text):\n",
    "    classification_result = classify(text)\n",
    "    return classification_result[0]\n",
    "\n",
    "# Function to process longer texts by splitting them into smaller chunks\n",
    "def process_long_text(text, max_chunk_length=512):\n",
    "    chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
    "    scores = []\n",
    "    for chunk in chunks:\n",
    "        chunk_scores = classify_text(chunk)\n",
    "        scores.extend(chunk_scores)\n",
    "    return scores\n",
    "\n",
    "# Apply text classification to each review text in the DataFrame and extract scores for all labels\n",
    "data_hedonic['classification_scores'] = data_hedonic['text'].apply(process_long_text)\n",
    "\n",
    "# Extract subjective and neutral scores separately\n",
    "subjective_scores = []\n",
    "neutral_scores = []\n",
    "for _, row in data_hedonic.iterrows():\n",
    "    label_scores = row['classification_scores']\n",
    "    subjective_score = next((entry['score'] for entry in label_scores if entry['label'] == 'SUBJECTIVE'), None)\n",
    "    neutral_score = next((entry['score'] for entry in label_scores if entry['label'] == 'NEUTRAL'), None)\n",
    "    subjective_scores.append(subjective_score)\n",
    "    neutral_scores.append(neutral_score)\n",
    "\n",
    "# Add the extracted scores as new columns to the DataFrame\n",
    "data_hedonic['subjective_score'] = subjective_scores\n",
    "data_hedonic['neutral_score'] = neutral_scores\n",
    "\n",
    "# Drop the 'classification_scores' column as it's no longer needed\n",
    "data_hedonic.drop(columns=['classification_scores'], inplace=True)\n",
    "\n",
    "print(data_hedonic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hedonic.to_csv(\"/Users/paulahofmann/Documents/Coding/Online-Review/FeaturePreperation/Data_with_Features/TestCasual_Subjective.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
